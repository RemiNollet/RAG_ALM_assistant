{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ec226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data link: https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/projects/447dd4/DIC.zip\n",
    "!mkdir -p ../data/\n",
    "!curl -L -o ../data/DIC.zip https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/projects/447dd4/DIC.zip\n",
    "!unzip -d ../data ../data/DIC.zip\n",
    "!rm ../data/DIC.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68654d5e",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441d19a-7f64-4916-ac8c-4f87d5111ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "documents = []\n",
    "#DIC_path = os.path.join(os.path.expanduser(\"~/RAG_ALM_assistant\"), \"data/DIC/*.pdf\")\n",
    "DIC_path = \"../data/DIC/*.pdf\"\n",
    "\n",
    "for file in glob.glob(DIC_path):\n",
    "    try:\n",
    "        loader = PyPDFLoader(file)  # Retourne une liste de document (un pour chaque page)\n",
    "        pages = loader.load()\n",
    "        for i, doc in enumerate(pages):\n",
    "            doc.metadata[\"dic_name\"] = str(file).split(\"/\")[3]\n",
    "            doc.metadata[\"page\"] = i + 1           # pages 1-based\n",
    "        documents += pages\n",
    "        \n",
    "    except Exception:\n",
    "        print(f\"Erreur survenue pour le fichier '{file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fda476",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(documents))\n",
    "print(\"metadata: \", documents[0].metadata)\n",
    "print(documents[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c273739",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\", \"\\n\\n\"],\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap = 60,\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents=documents)\n",
    "print(f\"{len(chunks)} chunks ont été créés par le splitter à partir du document PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf95f11",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5893900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def get_vectorstore(docs, model_name=\"intfloat/multilingual-e5-large\", normalize_embeddings=True):\n",
    "    encode_kwargs = {\"normalize_embeddings\": normalize_embeddings}\n",
    "    embedding = HuggingFaceEmbeddings(model_name=model_name, encode_kwargs=encode_kwargs)\n",
    "    vectore_store = Chroma.from_documents(documents=docs, embedding=embedding, persist_directory=\"../data/vector_store\")\n",
    "    return vectore_store\n",
    "\n",
    "vector_store = get_vectorstore(chunks)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type = 'similarity', search_kwargs={'k':5})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retriever.invoke(\"Qu'est ce que l'OPCVM?\")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(\"--- \")\n",
    "    print(f\"== Contenu du chunk {i} ==\\n{result.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e4dd36",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token=..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "model_config = transformers.AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code = True,\n",
    "    config = model_config,\n",
    "    device_map = 'auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e02ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline = pipeline(\n",
    "        'text-generation',\n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens = 4096,\n",
    "        do_sample = False,\n",
    "        return_full_text = False # Très important ! On ne veut pas le prompt initial\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efbf52",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc63a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"You are an assistant for question-answer tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\\n\"\n",
    "    \"Chat history:\\n{chat_history}\\n\\n\"\n",
    "    \"Context:\\n{context}\\n\\n\"\n",
    "    \"Question: {question}\\n\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\",\n",
    "    input_key=\"question\"\n",
    ")\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "def rag_pipeline(question: str):\n",
    "    result = qa_chain({\"question\": question})\n",
    "    answer = result[\"answer\"]\n",
    "    source_docs = result[\"source_documents\"]\n",
    "\n",
    "    # formatage simple des sources pour les DIC\n",
    "    sources = []\n",
    "    for d in source_docs:\n",
    "        m = d.metadata\n",
    "        sources.append({\n",
    "            \"dic_name\": m.get(\"dic_name\"),\n",
    "            \"page\": m.get(\"page\")\n",
    "        })\n",
    "\n",
    "    return answer, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b5881",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "Donnes moi des informations sur l'OPCVM. quand a t'il ete creer et a quoi ca sert?\n",
    "\"\"\"\n",
    "\n",
    "# Effectuer une requête\n",
    "answer, sources = rag_pipeline(query)\n",
    "\n",
    "print(\"Réponse :\\n\", answer)\n",
    "print(\"\\nSources :\")\n",
    "for s in sources:\n",
    "    print(f\"- {s['dic_name']} (page {s['page']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f6aa78-f875-4375-a711-1f37a81d9255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
